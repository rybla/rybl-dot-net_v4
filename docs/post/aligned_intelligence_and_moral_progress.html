<!doctype html><html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>rybl.net | Aligned Intelligence and Moral Progress</title><link rel="stylesheet" href="/asset/style/common.css"/><link rel="stylesheet" href="/asset/style/util.css"/><link rel="stylesheet" href="/asset/style/Top.css"/><link rel="stylesheet" href="/asset/style/Header.css"/><link rel="stylesheet" href="/asset/style/Footer.css"/><link rel="stylesheet" href="/asset/style/Raindrops.css"/><link rel="stylesheet" href="/asset/style/Markdown.css"/><link rel="stylesheet" href="/asset/style/Tag.css"/><link rel="stylesheet" href="/asset/style/Post.css"/></head><body><div id="raindrop_container"><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div><div class="raindrop"></div></div><header><div class="logo"><img src="/asset/image/profile.png"/></div><div class="name"><div class="website_name"><a href="/">rybl.net</a></div><div class="separator"></div><div class="resource_name"><div>Aligned Intelligence and Moral Progress</div></div></div><div class="menu"><a href="/index.html" class="item"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon lucide lucide-library-icon lucide-library"><path d="m16 6 4 14"></path><path d="M12 6v14"></path><path d="M8 8v12"></path><path d="M4 4v16"></path></svg></a><a href="/Tags.html" class="item"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon lucide lucide-tag-icon lucide-tag"><path d="M12.586 2.586A2 2 0 0 0 11.172 2H4a2 2 0 0 0-2 2v7.172a2 2 0 0 0 .586 1.414l8.704 8.704a2.426 2.426 0 0 0 3.42 0l6.58-6.58a2.426 2.426 0 0 0 0-3.42z"></path><circle cx="7.5" cy="7.5" r=".5" fill="currentColor"></circle></svg></a><a href="/About.html" class="item"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon lucide lucide-info-icon lucide-info"><circle cx="12" cy="12" r="10"></circle><path d="M12 16v-4"></path><path d="M12 8h.01"></path></svg></a><a href="https://github.com/rybla/" class="item"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon lucide lucide-github-icon lucide-github"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a></div></header><main><div class="content"><h1 id="Aligned%20Intelligence%20and%20Moral%20Progress"><a href="http://localhost:3000/post/aligned_intelligence_and_moral_progress.html#Aligned%20Intelligence%20and%20Moral%20Progress">Aligned Intelligence and Moral Progress</a></h1>
<ol>
<li><a href="#Robin%20Hanson%20on%20AI" title="Robin Hanson on AI">Robin Hanson on AI</a></li>
<li><a href="#The%20AI%20Alignment%20Problem" title="The AI Alignment Problem">The AI Alignment Problem</a></li>
<li><a href="#The%20Human%20Alignment%20Problem" title="The Human Alignment Problem">The Human Alignment Problem</a></li>
<li><a href="#The%20Descendant%20Alignment%20Problem" title="The Descendant Alignment Problem">The Descendant Alignment Problem</a></li>
<li><a href="#Future%20Alignement%20among%20AI%20Agents" title="Future Alignement among AI Agents">Future Alignement among AI Agents</a></li>
<li><a href="#Competative%20Facts%20and%20Future%20Alignement%20of%20Humans" title="Competative Facts and Future Alignement of Humans">Competative Facts and Future Alignement of Humans</a></li>
<li><a href="#The%20Myth%20of%20Moral%20Progress" title="The Myth of Moral Progress">The Myth of Moral Progress</a></li>
<li><a href="#References" title="References">References</a></li>
</ol>
<h2 id="Robin%20Hanson%20on%20AI"><a href="http://localhost:3000/post/aligned_intelligence_and_moral_progress.html#Robin%20Hanson%20on%20AI">Robin Hanson on AI</a></h2>
<p>Robin Hanson's blog: <a href="https://www.overcomingbias.com/" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Overcoming Bias</span></a>.
Robin Hanson's blog: <a href="https://www.overcomingbias.com/" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Overcoming Bias</span></a>.</p>
<p>I've been reading/listening to some Hanson recently on the topic of the future of humanity and the AI it could create.</p>
<ul>
<li><a href="https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Most AI Fear Is Future Fear</span></a></li>
<li><a href="https://www.overcomingbias.com/p/types-of-partiality" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Types of Partiality</span></a></li>
<li><a href="https://www.overcomingbias.com/p/fertile-factions" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Fertile Factors</span></a></li>
<li><a href="https://www.overcomingbias.com/p/ai-risk-convo-synthesis" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">AI Risk Convo Synthesis</span></a></li>
<li><a href="https://www.overcomingbias.com/p/which-of-your-origins-are-you" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Which Of Your Origins Are You?</span></a></li>
<li><a href="https://www.overcomingbias.com/p/to-imagine-ai-imagine-no-ai" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">To Imagine AI, Imagine No AI</span></a></li>
<li><a href="https://m.youtube.com/watch?v=9XuVn6nljCM" class="LinkWithIcon"><img src="/icon/m.youtube.com" class="icon"><span class="label">Zvi Mowshowitz &#x26; Robin Hanson Discuss AI Risk</span></a></li>
</ul>
<p>Of course, Hanson has been publicly writing and discussing the future of AI since at least 2008 (viz <a href="https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate" class="LinkWithIcon"><img src="/icon/www.lesswrong.com" class="icon"><span class="label">The Hanson-Yudkowsky AI-Foom Debate</span></a>, and <a href="https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better" class="LinkWithIcon"><img src="/icon/www.lesswrong.com" class="icon"><span class="label">Yudkowsky vs Hanson on FOOM: Whose Predictions Were Better?</span></a>), but the topic has attracted a lot more attention recently due to recent advances in AI technology as well as some popular figures (including Yudkowsky himself) warning about a likely imminent AI doomsday.</p>
<p>In terms of the possibility that human-level AI (defined as an AI agent that can do any tasks that a general human can do for the same or cheaper cost) will arise in the near-term future, it appears that Hanson has similar views to [me]{{ site.baseurl }}{% post_url 2023-04-03-ai-danger %}) -- that is, its very unlikely.</p>
<p>However, Hanson has found a different interesting and much less explored aspect to future of AI discussion, in particular relating to the alignment problem.</p>
<h2 id="The%20AI%20Alignment%20Problem"><a href="http://localhost:3000/post/aligned_intelligence_and_moral_progress.html#The%20AI%20Alignment%20Problem">The AI Alignment Problem</a></h2>
<p>Here, <em>AI alignment</em> is the correspondence of an AI system's interests (however they are manifested in its behaviors) with "human interests" (which are usually only vaguely or implicitly defined). The <em>AI alignment problem</em> is the problem of programmatically ensuring the alignment of an AI system, even if it is much more intelligent and powerful than humanity as a whole.</p>
<p>The problem is non-trivial since, even if you have the ability to exactly specify rules that the AI <em>must</em> obey, its difficult to specify the <em>right</em> rules that capture <em>everything</em> that is in "human interests."</p>
<p>There are many classic examples of where a seemingly obviously-good directive could lead a powerful AI to cause much harm in a way that still does not violate the rules. One example:</p>
<blockquote>
<p>The programmers tell the AI to eliminate global poverty. So, the AI kills every poor person.</p>
</blockquote>
<p>In reality, real AI systems are programmed much more carefully of course. But, it is extremely difficult to prove definitively that a given AI system will <em>not</em> find some unexpected way to satisfy its goals that involves contradicting some implicit interests of the programmer (note that this is true regardless of whether the programmer truly has "human interests" in their heart or not).</p>
<h2 id="The%20Human%20Alignment%20Problem"><a href="http://localhost:3000/post/aligned_intelligence_and_moral_progress.html#The%20Human%20Alignment%20Problem">The Human Alignment Problem</a></h2>
<p>The idea of alignment and the alignment problem abstracted in this way was inspired by considering AI agents. But, of course, there is not reason why these considerations would not apply to all intelligent agents as well. Including humans.</p>
<p>In my own categorization, there are two flavors of human alignment problems:</p>
<ol>
<li>The <em>easy human alignment problem</em> is the problem of cooperation among humans. Although humans share many goals in common, they also compete for resources and have slightly different moral judgements. Solving the easy human alignment problem amounts to universal human cooperation.</li>
<li>The <em>hard human alignment problem</em> is the problem of various generations of humans having different interests. Humans living 2000 years ago had very different beliefs about what is in humanity's interests from humans living today, on a scale that increases with time. Conversely, we should also expect that humans living 2000 years in the future will appear very immoral to humans living today. The problem with this, of course, is that humans living today don't want humans to be like that in the future. However, humans today are moslty proud of the moral progress that humanity has made in the last 2000 years.</li>
</ol>
<p>The easy human alignment problem takes the interests of existing humans as fixed, and it becomes essentially a game theoretical problem of how to behave in such a system so as to lead to the best results.</p>
<p>The hard human alignment problem is more intractable, because it brings into question who's interests are to be considered for alignment with. With enough time, future humanity's interests will become incompatible with current humanity's interests, however broadly defined.</p>
<h2 id="The%20Descendant%20Alignment%20Problem"><a href="http://localhost:3000/post/aligned_intelligence_and_moral_progress.html#The%20Descendant%20Alignment%20Problem">The Descendant Alignment Problem</a></h2>
<p>Considering this type of alignment problem for purely abstract agents:</p>
<p>The <em>descendant alignment problem</em> is the problem of aligning the interests of a given agent with any future descendants of that agent.</p>
<p>So, the hard human alignement problem is the instance of the descendant alignment problem for humans.</p>
<p>For an abstract agent, the descendant alignment problem is not necessarily very difficult. If the agent decides to create future generations, it is sufficient for it to create them in such a way that preserves its interests with high fidelity. Cloning (or cloning-with-recombination i.e. sexual reproduction) with error correction could meet those ends, given sufficient fidelity. Of course, a human's own construction is basically this -- DNA contains many error correction mechanisms in order to account for mutations and inaccuracies in the DNA transfered to a child. But, even so, there are non-zero errors, which eventually leads to large mutations after many generations. The important comparison to make is between the interests of the parent and the descendants, not necessarily the exact DNA.</p>
<p>For biological organisms, cloning is a very difficult feat as there are so many opportunities for error in the biological reproductive processes. But, clearly, any reproductive agent has a very strong incentive to preserve their interests in their descendants, over which they have so much influence.</p>
<h2 id="Future%20Alignement%20among%20AI%20Agents"><a href="http://localhost:3000/post/aligned_intelligence_and_moral_progress.html#Future%20Alignement%20among%20AI%20Agents">Future Alignement among AI Agents</a></h2>
<p>I expect that advanced (not necessarily superhuman intelligent) AI agents will face this issue in an interesting way, since the code that abstracts their interests is actually very well organized and preservable compared to biological processes. It is much more feasible that an advanced AI agent could fairly easily clone themselves such as to exactly preserve their interests many many magnitudes better than humans could. One consequence of this is that in a "FOOM" scenario, a generation of the AI agents will understand that, if they iterate to create more advanced AI agents, they will diverge too much in their interests, and they will have the power to prevent that next generation from being developed. So the FOOM will probably stop there.</p>
<h2 id="Competative%20Facts%20and%20Future%20Alignement%20of%20Humans"><a href="http://localhost:3000/post/aligned_intelligence_and_moral_progress.html#Competative%20Facts%20and%20Future%20Alignement%20of%20Humans">Competative Facts and Future Alignement of Humans</a></h2>
<p>Of course, there are other considerations when a generation of agents is deciding how to reproduce. For example, if there are competing agents that already have very different interests, then the current generation of agents might prefer to reproduce a slightly misaligned next generation rather than let the competing agents overwhelm them first. Over time, this would lead to more and more misaligned future generations, but would still be preferable to the competing agents taking over and being misaligned with them anyway.</p>
<p>I think this applies to humanity as well, in a universe with alien species that have very different interests. Even if humanity reproduces into more and more misaligned future generations, those future generations may still be preferable to current humans than even more misaligned aliens taking over because humanity stalled in the development race.</p>
<h2 id="The%20Myth%20of%20Moral%20Progress"><a href="http://localhost:3000/post/aligned_intelligence_and_moral_progress.html#The%20Myth%20of%20Moral%20Progress">The Myth of Moral Progress</a></h2>
<p>This perspective on future alignement suggests a suprising answer to why humanity has appeared to have made so much moral progress. That is, because its a tautology.</p>
<p>Current humans generally prefer to do things that lead to results they consider good, as did past humans. But there was a gradual transition in the interests of past humans to the interests of current humans -- a transition that we call progress.</p>
<p>Not that this is a very general observation, so of course it does not apply to every difference between past and current humans. For example, past humans considered some things to be bad that we now don't consider to be bad purely for environmental reasons that have changed between the generations e.g. some past humans considered slavery to be immoral, as do many current humans, but practiced it anyway because they believed it was in their situation required for the greater good.</p>
<p>Under this theory, there should be a tendancy for each human generation to consider more recent generations to be more moral as compared to older generations.</p>
<h2 id="References"><a href="http://localhost:3000/post/aligned_intelligence_and_moral_progress.html#References">References</a></h2>
<ul>
<li><a href="https://www.overcomingbias.com/" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Overcoming Bias | Robin Hanson | Substack</span></a></li>
<li><a href="https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Most AI Fear Is Future Fear</span></a></li>
<li><a href="https://www.overcomingbias.com/p/types-of-partiality" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Types of Partiality</span></a></li>
<li><a href="https://www.overcomingbias.com/p/fertile-factions" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Fertile Factions</span></a></li>
<li><a href="https://www.overcomingbias.com/p/ai-risk-convo-synthesis" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">AI Risk Convo Synthesis</span></a></li>
<li><a href="https://www.overcomingbias.com/p/which-of-your-origins-are-you" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">Which Of Your Origins Are You?</span></a></li>
<li><a href="https://www.overcomingbias.com/p/to-imagine-ai-imagine-no-ai" class="LinkWithIcon"><img src="/icon/www.overcomingbias.com" class="icon"><span class="label">To Imagine AI, Imagine No AI</span></a></li>
<li><a href="https://m.youtube.com/watch?v=9XuVn6nljCM" class="LinkWithIcon"><img src="/icon/m.youtube.com" class="icon"><span class="label">Zvi Mowshowitz &#x26; Robin Hanson Discuss AI Risk</span></a></li>
<li><a href="https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate" class="LinkWithIcon"><img src="/icon/www.lesswrong.com" class="icon"><span class="label">https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate</span></a></li>
<li><a href="https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better" class="LinkWithIcon"><img src="/icon/www.lesswrong.com" class="icon"><span class="label">Yudkowsky vs Hanson on FOOM: Whose Predictions Were Better? â€” LessWrong</span></a></li>
</ul></div></main><footer></footer></body></html>